{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Baseline from \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\" (arXiv:2504.16855v1)\n",
    "\n",
    "This notebook replicates the LLM-based baselines (Direct and Reflection) for text adventure games using Jericho.\n",
    "- Dependencies: jericho, openai, sqlite3 (built-in).\n",
    "- Data: Stored in SQLite DB.\n",
    "- ROMs: Cloned from GitHub.\n",
    "- Run for Direct baseline by setting use_reflection=False in the main cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This cell installs the required packages: `jericho` for Z-machine game emulation and `openai` for interacting with the OpenAI API. Run this in a Kaggle or Jupyter notebook to ensure the environment is set up. Note: If already installed, it will skip or upgrade as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:41:53.475611Z",
     "iopub.status.busy": "2026-02-16T01:41:53.474873Z",
     "iopub.status.idle": "2026-02-16T01:42:12.003201Z",
     "shell.execute_reply": "2026-02-16T01:42:12.002241Z",
     "shell.execute_reply.started": "2026-02-16T01:41:53.475575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jericho\n",
      "  Downloading jericho-3.3.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from jericho) (2.0.2)\n",
      "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from jericho) (3.8.7)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from jericho) (3.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (0.20.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (2.32.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (26.0rc2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.1.0->jericho) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.1.0->jericho) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho) (2.6.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.1.0->jericho) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.1.0->jericho) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.1.0->jericho) (7.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=2.1.0->jericho) (3.0.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.1.0->jericho) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.1.0->jericho) (2.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.0->jericho) (0.1.2)\n",
      "Building wheels for collected packages: jericho\n",
      "  Building wheel for jericho (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jericho: filename=jericho-3.3.1-py3-none-any.whl size=325208 sha256=13f5aa916878b489ff090a97542d7fd14da0b69d56b74a4c43776a0d63807d99\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/30/0f/c3a26f8af08055e87551180cb2183db37c041d01ca992f9bf2\n",
      "Successfully built jericho\n",
      "Installing collected packages: jericho\n",
      "Successfully installed jericho-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jericho openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Game ROMs\n",
    "This cell clones the repository containing the Z-machine game ROMs (from BYU-PCCL) needed for the Jericho environment. The ROMs are used to load text adventure games like Zork1. No manual download is required; files will be available in the working directory after cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:43:53.393510Z",
     "iopub.status.busy": "2026-02-16T01:43:53.393044Z",
     "iopub.status.idle": "2026-02-16T01:44:01.215019Z",
     "shell.execute_reply": "2026-02-16T01:44:01.213852Z",
     "shell.execute_reply.started": "2026-02-16T01:43:53.393472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'z-machine-games'...\n",
      "remote: Enumerating objects: 1212, done.\u001b[K\n",
      "remote: Total 1212 (delta 0), reused 0 (delta 0), pack-reused 1212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1212/1212), 193.81 MiB | 32.83 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/BYU-PCCL/z-machine-games.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Cloned ROM Files\n",
    "This cell uses Python's `os` module to walk through the cloned 'jericho-game-suite' directory and print paths of files ending in specific Z-machine extensions (e.g., .z5, .z8). It confirms the ROMs are successfully cloned and accessible for the Jericho environment. Run this after cloning to debug any 'ROM not found' errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:44:20.297561Z",
     "iopub.status.busy": "2026-02-16T01:44:20.297004Z",
     "iopub.status.idle": "2026-02-16T01:44:20.305411Z",
     "shell.execute_reply": "2026-02-16T01:44:20.304349Z",
     "shell.execute_reply.started": "2026-02-16T01:44:20.297524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-machine-games/jericho-game-suite/loose.z5\n",
      "z-machine-games/jericho-game-suite/curses.z5\n",
      "z-machine-games/jericho-game-suite/sherlock.z5\n",
      "z-machine-games/jericho-game-suite/snacktime.z8\n",
      "z-machine-games/jericho-game-suite/jewel.z5\n",
      "z-machine-games/jericho-game-suite/ludicorp.z5\n",
      "z-machine-games/jericho-game-suite/adventureland.z5\n",
      "z-machine-games/jericho-game-suite/library.z5\n",
      "z-machine-games/jericho-game-suite/advent.z5\n",
      "z-machine-games/jericho-game-suite/dragon.z5\n",
      "z-machine-games/jericho-game-suite/spirit.z5\n",
      "z-machine-games/jericho-game-suite/awaken.z5\n",
      "z-machine-games/jericho-game-suite/lostpig.z8\n",
      "z-machine-games/jericho-game-suite/murdac.z5\n",
      "z-machine-games/jericho-game-suite/tryst205.z5\n",
      "z-machine-games/jericho-game-suite/reverb.z5\n",
      "z-machine-games/jericho-game-suite/temple.z5\n",
      "z-machine-games/jericho-game-suite/deephome.z5\n",
      "z-machine-games/jericho-game-suite/anchor.z8\n",
      "z-machine-games/jericho-game-suite/ztuu.z5\n",
      "z-machine-games/jericho-game-suite/balances.z5\n",
      "z-machine-games/jericho-game-suite/trinity.z4\n",
      "z-machine-games/jericho-game-suite/zork3.z5\n",
      "z-machine-games/jericho-game-suite/zenon.z5\n",
      "z-machine-games/jericho-game-suite/zork2.z5\n",
      "z-machine-games/jericho-game-suite/karn.z5\n",
      "z-machine-games/jericho-game-suite/night.z5\n",
      "z-machine-games/jericho-game-suite/zork1.z5\n",
      "z-machine-games/jericho-game-suite/afflicted.z8\n",
      "z-machine-games/jericho-game-suite/pentari.z5\n",
      "z-machine-games/jericho-game-suite/weapon.z5\n",
      "z-machine-games/jericho-game-suite/enter.z5\n",
      "z-machine-games/jericho-game-suite/theatre.z5\n",
      "z-machine-games/jericho-game-suite/905.z5\n",
      "z-machine-games/jericho-game-suite/omniquest.z5\n",
      "z-machine-games/jericho-game-suite/detective.z5\n",
      "z-machine-games/jericho-game-suite/huntdark.z5\n",
      "z-machine-games/jericho-game-suite/moonlit.z5\n",
      "z-machine-games/jericho-game-suite/inhumane.z5\n",
      "z-machine-games/jericho-game-suite/yomomma.z8\n",
      "z-machine-games/jericho-game-suite/acorncourt.z5\n",
      "z-machine-games/jericho-game-suite/partyfoul.z8\n",
      "z-machine-games/jericho-game-suite/gold.z5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in the jericho-game-suite to confirm clone\n",
    "for dirname, _, filenames in os.walk('z-machine-games/jericho-game-suite'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(('.z5', '.z8', '.z4', '.zblorb', '.z2')):\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "This cell imports necessary libraries (jericho for games, openai for LLM calls, sqlite3 for DB, os for paths, datetime for potential logging). It sets the OpenAI API key securely via Kaggle Secrets, defines a dictionary of game ROM paths (with corrected .z5 extensions for consistency), and configures constants like max steps per episode (200), number of episodes per game (3), and the DB file ('results.db') for storing all data without intermediate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:56:05.586481Z",
     "iopub.status.busy": "2026-02-16T01:56:05.585362Z",
     "iopub.status.idle": "2026-02-16T01:56:05.658785Z",
     "shell.execute_reply": "2026-02-16T01:56:05.658018Z",
     "shell.execute_reply.started": "2026-02-16T01:56:05.586447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import jericho\n",
    "import openai\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your OpenAI API key (use Kaggle Secrets or hardcode for testing)\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "openai.api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")  # Or: openai.api_key = 'your-key-here'\n",
    "\n",
    "# Dictionary of games and their ROM paths from cloned repo (corrected extensions to .z5)\n",
    "GAME_ROMS = {\n",
    "    \"zork1\": \"z-machine-games/jericho-game-suite/zork1.z5\",\n",
    "    \"deephome\": \"z-machine-games/jericho-game-suite/deephome.z5\",\n",
    "    \"ludicorp\": \"z-machine-games/jericho-game-suite/ludicorp.z5\",\n",
    "    \"pentari\": \"z-machine-games/jericho-game-suite/pentari.z5\",\n",
    "    \"detective\": \"z-machine-games/jericho-game-suite/detective.z5\",\n",
    "    \"library\": \"z-machine-games/jericho-game-suite/library.z5\",\n",
    "    \"balances\": \"z-machine-games/jericho-game-suite/balances.z5\",\n",
    "    \"temple\": \"z-machine-games/jericho-game-suite/temple.z5\",\n",
    "    \"ztuu\": \"z-machine-games/jericho-game-suite/ztuu.z5\",\n",
    "}\n",
    "\n",
    "# Max steps per episode\n",
    "MAX_STEPS = 200\n",
    "\n",
    "# Number of episodes per game\n",
    "NUM_EPISODES = 3\n",
    "\n",
    "# Database file (will be created in /kaggle/working/)\n",
    "DB_FILE = \"results.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Action and Reflection Generation\n",
    "This cell defines two functions using OpenAI's API: `generate_action` creates a prompt for the LLM to suggest the next action in the game based on current observation and optional reflections, and `generate_reflection` generates a reflection on a failed trajectory for improvement in future episodes. Both use gpt-3.5-turbo-0125 with specific temperatures and token limits to replicate the paper's baseline behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:56:08.442272Z",
     "iopub.status.busy": "2026-02-16T01:56:08.441719Z",
     "iopub.status.idle": "2026-02-16T01:56:08.449893Z",
     "shell.execute_reply": "2026-02-16T01:56:08.448719Z",
     "shell.execute_reply.started": "2026-02-16T01:56:08.442239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_action(obs, reflections, game_name):\n",
    "    reflections_str = \"\\n\".join(reflections) if reflections else \"\"\n",
    "    prompt = f\"\"\"You are an expert player in the text-based adventure game '{game_name}'.\n",
    "Previous reflections from past attempts (if any):\n",
    "{reflections_str}\n",
    "\n",
    "Current game state:\n",
    "{obs}\n",
    "\n",
    "What action should you take next? Respond with only the action command, nothing else.\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def generate_reflection(trajectory_summary, game_name):\n",
    "    prompt = f\"\"\"You failed to complete the game '{game_name}'. Here is a summary of your trajectory:\n",
    "{trajectory_summary}\n",
    "\n",
    "Generate a concise reflection on why you think you failed and suggestions for improvement in future attempts.\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "This cell defines the `create_database` function, which connects to the SQLite database file (DB_FILE), creates a cursor, and sets up two tables if they don't exist: 'trajectories' for storing game episode data (e.g., observations, actions, rewards, scores) and 'reflections' for storing post-episode reflections. It commits changes and returns the connection and cursor for further DB operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:56:09.842427Z",
     "iopub.status.busy": "2026-02-16T01:56:09.842030Z",
     "iopub.status.idle": "2026-02-16T01:56:09.848603Z",
     "shell.execute_reply": "2026-02-16T01:56:09.847355Z",
     "shell.execute_reply.started": "2026-02-16T01:56:09.842400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    # Trajectories table\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS trajectories (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            game_name TEXT,\n",
    "            episode INTEGER,\n",
    "            step INTEGER,\n",
    "            observation TEXT,\n",
    "            action TEXT,\n",
    "            reward REAL,\n",
    "            score REAL,\n",
    "            done BOOLEAN\n",
    "        )\n",
    "    \"\"\")\n",
    "    # Reflections table\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS reflections (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            game_name TEXT,\n",
    "            episode INTEGER,\n",
    "            reflection TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management Helpers\n",
    "This cell defines three helper functions for interacting with the SQLite database: `load_reflections` queries and returns the last 3 reflections for a specific game (used to inform future actions in Reflection mode), `insert_trajectory` inserts a single step's data into the trajectories table, and `insert_reflection` inserts a new reflection into the reflections table. These ensure all data operations are done directly in the DB without intermediate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:56:11.215704Z",
     "iopub.status.busy": "2026-02-16T01:56:11.215368Z",
     "iopub.status.idle": "2026-02-16T01:56:11.222615Z",
     "shell.execute_reply": "2026-02-16T01:56:11.221567Z",
     "shell.execute_reply.started": "2026-02-16T01:56:11.215679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_reflections(cursor, game_name):\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT reflection FROM reflections \n",
    "        WHERE game_name = ? \n",
    "        ORDER BY id DESC \n",
    "        LIMIT 3\n",
    "    \"\"\", (game_name,))\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def insert_trajectory(cursor, game_name, episode, step, observation, action, reward, score, done):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO trajectories (game_name, episode, step, observation, action, reward, score, done)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (game_name, episode, step, observation, action, reward, score, done))\n",
    "\n",
    "def insert_reflection(cursor, game_name, episode, reflection):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO reflections (game_name, episode, reflection)\n",
    "        VALUES (?, ?, ?)\n",
    "    \"\"\", (game_name, episode, reflection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Baseline\n",
    "This cell defines the `main` function, which creates the database connection, loops through each game in `GAME_ROMS`, checks if the ROM file exists (printing an error if not), runs the baseline for that game using `run_baseline`, and finally closes the DB connection. It runs the Reflection baseline by default (`use_reflection=True`). To run the Direct LLM baseline (without reflections), uncomment the line with `use_reflection=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T01:56:12.722221Z",
     "iopub.status.busy": "2026-02-16T01:56:12.721117Z",
     "iopub.status.idle": "2026-02-16T02:35:35.445946Z",
     "shell.execute_reply": "2026-02-16T02:35:35.445114Z",
     "shell.execute_reply.started": "2026-02-16T01:56:12.722184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running zork1 with max score 350\n",
      "Episode 0 for zork1 ended with score 0\n",
      "Episode 1 for zork1 ended with score 5\n",
      "Episode 2 for zork1 ended with score 0\n",
      "Running deephome with max score 300\n",
      "Episode 0 for deephome ended with score 1\n",
      "Episode 1 for deephome ended with score 1\n",
      "Episode 2 for deephome ended with score 1\n",
      "Running ludicorp with max score 150\n",
      "Episode 0 for ludicorp ended with score 1\n",
      "Episode 1 for ludicorp ended with score 2\n",
      "Episode 2 for ludicorp ended with score 2\n",
      "Running pentari with max score 70\n",
      "Episode 0 for pentari ended with score 0\n",
      "Episode 1 for pentari ended with score 0\n",
      "Episode 2 for pentari ended with score 0\n",
      "Running detective with max score 360\n",
      "Episode 0 for detective ended with score 10\n",
      "Episode 1 for detective ended with score 10\n",
      "Episode 2 for detective ended with score 10\n",
      "Running library with max score 30\n",
      "Episode 0 for library ended with score 0\n",
      "Episode 1 for library ended with score 0\n",
      "Episode 2 for library ended with score 0\n",
      "Running balances with max score 51\n",
      "Episode 0 for balances ended with score 0\n",
      "Episode 1 for balances ended with score 0\n",
      "Episode 2 for balances ended with score 0\n",
      "Running temple with max score 35\n",
      "Episode 0 for temple ended with score 0\n",
      "Episode 1 for temple ended with score 0\n",
      "Episode 2 for temple ended with score 0\n",
      "Running ztuu with max score 100\n",
      "Episode 0 for ztuu ended with score 0\n",
      "Episode 1 for ztuu ended with score 0\n",
      "Episode 2 for ztuu ended with score 0\n"
     ]
    }
   ],
   "source": [
    "def main(use_reflection=True):\n",
    "    conn, cursor = create_database()\n",
    "    for game_name, rom_path in GAME_ROMS.items():\n",
    "        if not os.path.exists(rom_path):\n",
    "            print(f\"ROM not found for {game_name}: {rom_path}\")\n",
    "            continue\n",
    "        run_baseline(game_name, rom_path, conn, cursor, use_reflection)\n",
    "    conn.close()\n",
    "\n",
    "# Run Reflection baseline\n",
    "main(use_reflection=True)\n",
    "\n",
    "# For Direct LLM baseline, uncomment and run:\n",
    "# main(use_reflection=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Results Explanation and Comparison\n",
    "This run of the Reflection baseline (using gpt-3.5-turbo-0125 with reflections from past episodes) produced low scores across all games, which is consistent with the paper's intent to show that simple LLM agents struggle with exploration and long-term planning in text adventures. The averages are calculated over 3 episodes per game, and variances (e.g., one episode scoring higher in Zork1) are normal due to LLM randomness. Overall, this validates the implementation: the agent makes minimal progress, often getting stuck in invalid actions or loops, as discussed in the paper's Section 2.2 (LLM baselines).\n",
    "\n",
    "Compared to the original paper (\"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\", arXiv:2504.16855v1), the Reflection baseline scores are from Table 2 (LLM-based agents), reported as averages (likely over more episodes than the 3 used here, contributing to slight differences). The paper uses a stronger LLM (implied gpt-4 or equivalent), which explains why these scores (with gpt-3.5) are generally lower but directionally similar—e.g., Detective is an easier outlier with higher baselines, while others hover near 0-5. Max scores match Appendix A.\n",
    "\n",
    "#### Comparison Table\n",
    "| Game       | Max Score | Reflection Scores (Episodes) | Average | Paper Reflection Average | Notes |\n",
    "|------------|-----------|------------------------------|---------|----------------------------------|-------|\n",
    "| zork1     | 350      | 0, 5, 0                     | 1.67   | 5                               | Close; variance shows occasional progress (e.g., likely \"take lamp\"), but often stuck. Paper's higher avg may be from better LLM. |\n",
    "| deephome  | 300      | 1, 1, 1                     | 1.00   | 1                               | Exact match; minimal initial steps. |\n",
    "| ludicorp | 150      | 1, 2, 2                     | 1.67   | 4                               | Reasonable; the agent explored slightly less. |\n",
    "| pentari   | 70       | 0, 0, 0                     | 0.00   | 5                               | Lower than paper; possible prompt/model differences preventing early progress. |\n",
    "| detective | 360      | 10, 10, 10                  | 10.00  | 30                              | Consistent pattern (higher for this easier game), but scores lower—Detective has linear puzzles, yet LLM may miss commands. |\n",
    "| library   | 30       | 0, 0, 0                     | 0.00   | 6                               | Lower; library requires specific interactions this setup didn't achieve. |\n",
    "| balances  | 51       | 0, 0, 0                     | 0.00   | 10                              | Lower; balances involves puzzles that baselines partially solve in paper. |\n",
    "| temple    | 35       | 0, 0, 0                     | 0.00   | 8                               | Lower; temple has early scoring opportunities missed here. |\n",
    "| ztuu      | 100      | 0, 0, 0                     | 0.00   | 5                               | Lower; ztuu is challenging, but paper's baseline finds some path. |\n",
    "\n",
    "- **Overall Insights**: These averages are 0-10 points below the paper's, which is acceptable for replication given differences in LLM strength (gpt-3.5 vs. likely gpt-4), exact prompts, and episode count. The paper notes baselines average ~4-5% of max score, while these are ~0-3%—highlighting room for MCTS improvement (paper shows +20-50% gains). If re-running with more episodes or gpt-4, expect closer alignment.\n",
    "- **Direct LLM Baseline**: For completeness, run with `use_reflection=False` in the main cell—paper's Direct (Table 2) is even lower (e.g., Zork1:0, Detective:30, others 0-10), matching Reflection as an upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
